{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import gym\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global agent parameters \n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "TRAINING_EPISODES = 25_000\n",
    "SHOW_EVERY = 2_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#Â Define the mountain car environment \n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "# Observation and action space \n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(f\"The observation space: {obs_space}\")\n",
    "print(f\"The action space: {action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to define a Q table. This is the quality of every possible action given every possible state. In most cases we will not know the bounds of our observation space. But, this gym enviroment tells us them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "# Maximum position and momentum values\n",
    "print(obs_space.high)\n",
    "# Minimum position and momentum values\n",
    "print(obs_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This observation space is continuous, thus we cannot make a table out of it. We must discretise the observation space to make a Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wanted position observations \n",
    "pos_obs = 20\n",
    "# Also find the position step\n",
    "dx = (obs_space.high[0] - obs_space.low[0])/pos_obs\n",
    "\n",
    "# Define wanted momentum observations \n",
    "mom_obs = 20 \n",
    "# Also find the momentum step\n",
    "dp = (obs_space.high[1] - obs_space.low[1])/mom_obs\n",
    "\n",
    "# Fill a matrix with all possible position and momentum positions after \n",
    "# discretisation\n",
    "disc_pos = np.arange(obs_space.low[0],\n",
    "                     obs_space.high[0] + dx,\n",
    "                     dx)\n",
    "disc_mom = np.arange(obs_space.low[1],\n",
    "                     obs_space.high[1] + dp,\n",
    "                     dp)\n",
    "# Find the discretised space that is a combination of all of these\n",
    "disc_space = np.array(np.meshgrid(disc_pos, disc_mom))\n",
    "\n",
    "# The Q table dimension will be a tensor of this with the action space dimension\n",
    "# Initialise the Q table with zeros \n",
    "q_table = np.zeros((pos_obs,\n",
    "                    mom_obs,\n",
    "                    env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that will tell us corresponding index of the \n",
    "# discrete state we are in given the continous state\n",
    "def discrete_state_index(\n",
    "    continuous_state: tuple,\n",
    "    dx: float,\n",
    "    dp: float,\n",
    "):\n",
    "    disc_state_index = ((continuous_state - env.observation_space.low))//[dx, dp]\n",
    "    # Coerce to int type and return tuple\n",
    "    disc_state_index = tuple(disc_state_index.astype(int))\n",
    "    \n",
    "    return disc_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define a function for updating the Q values\n",
    "# Do this without epsilon initially. May go back to change this \n",
    "def new_q_value(\n",
    "    q_table: np.array,\n",
    "    current_state_index: float,\n",
    "    new_state_index: float,\n",
    "    action,\n",
    "    reward: float,\n",
    "    learning_rate: float,\n",
    "    discount_factor: float,\n",
    "):\n",
    "    new_q_value = (q_table[current_state_index][action] +\n",
    "                   learning_rate * (\n",
    "                       reward + \n",
    "                       discount_factor * np.max(q_table[new_state_index]) -\n",
    "                       q_table[current_state_index][action]\n",
    "                   )\n",
    "    )\n",
    "    \n",
    "    return new_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is episode 0\n",
      "This is episode 2000\n",
      "This is episode 4000\n",
      "This is episode 6000\n",
      "This is episode 8000\n",
      "This is episode 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/CurtisIrvine/Documents/Q_learning/mountain_car.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/CurtisIrvine/Documents/Q_learning/mountain_car.ipynb#ch0000009?line=19'>20</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_table[current_state_index])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/CurtisIrvine/Documents/Q_learning/mountain_car.ipynb#ch0000009?line=20'>21</a>\u001b[0m \u001b[39m# Take a step, find the new state, reward and if we are done\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/CurtisIrvine/Documents/Q_learning/mountain_car.ipynb#ch0000009?line=21'>22</a>\u001b[0m new_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/CurtisIrvine/Documents/Q_learning/mountain_car.ipynb#ch0000009?line=22'>23</a>\u001b[0m \u001b[39m# Render \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/CurtisIrvine/Documents/Q_learning/mountain_car.ipynb#ch0000009?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m render:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py:49\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=38'>39</a>\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=39'>40</a>\u001b[0m \n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=40'>41</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=46'>47</a>\u001b[0m \u001b[39m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=47'>48</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=48'>49</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=49'>50</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/time_limit.py?line=50'>51</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py?line=35'>36</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/env_checker.py:41\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/env_checker.py?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m passive_env_step_check(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/env_checker.py?line=39'>40</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/wrappers/env_checker.py?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:128\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=125'>126</a>\u001b[0m velocity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(velocity, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_speed, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_speed)\n\u001b[1;32m    <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=126'>127</a>\u001b[0m position \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m velocity\n\u001b[0;32m--> <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=127'>128</a>\u001b[0m position \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mclip(position, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_position, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_position)\n\u001b[1;32m    <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=128'>129</a>\u001b[0m \u001b[39mif\u001b[39;00m position \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_position \u001b[39mand\u001b[39;00m velocity \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/CurtisIrvine/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py?line=129'>130</a>\u001b[0m     velocity \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:6\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Begin learning\n",
    "for episode in range(TRAINING_EPISODES):\n",
    "    # Check if we want to observe this episode\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        print(f\"This is episode {episode}\")\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "        \n",
    "        \n",
    "    # Iterate over the episode\n",
    "    done = False\n",
    "    # Find what state we start in \n",
    "    current_state_index = discrete_state_index(continuous_state=env.reset(),\n",
    "                                            dx=dx,\n",
    "                                            dp=dp)\n",
    "    \n",
    "    while not done:\n",
    "        # Find the best estimate action currently \n",
    "        action = np.argmax(q_table[current_state_index])\n",
    "        # Take a step, find the new state, reward and if we are done\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # Render \n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        #Check we didn't succeed\n",
    "        if not done:\n",
    "            # Find the index in the q table corresponding to the state\n",
    "            new_state_index = discrete_state_index(continuous_state=new_state,\n",
    "                                                    dx=dx,\n",
    "                                                    dp=dp)\n",
    "            # Update the q table \n",
    "            q_table[current_state_index][action] = new_q_value(\n",
    "                q_table=q_table,\n",
    "                current_state_index=current_state_index,\n",
    "                new_state_index=new_state_index,\n",
    "                action=action,\n",
    "                reward=reward,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                discount_factor=DISCOUNT_FACTOR\n",
    "            )\n",
    "            \n",
    "        # We also want to account for the simulation ending and assign max Q \n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            q_table[new_state_index][action] = 0\n",
    "            \n",
    "        # Update the state index \n",
    "        current_state_index = new_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
